{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                  # Manupulation of Multidimensional array and matrix\n",
    "import matplotlib.pyplot as plt     # For Visualisation\n",
    "import pandas as pd                 # Loading Dataset and Manupulation\n",
    "import seaborn as sns               # Plotting more attracting graphs and chart\n",
    "from numpy import array \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler # To Scale the data\n",
    "\n",
    "#For Upsampling to reduce Imbalance in dataset\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# For Splitting the Dataset into train-test and Performing Cross Validation Score\n",
    "from sklearn.model_selection import train_test_split,cross_val_score, KFold\n",
    "\n",
    "#For Performing the Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#For Performing Random Forest Classifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#For Performing Naive Bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#For Performing KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#For Performing Support Vector Machine\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "\n",
    "#For Performing Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#For Performing Voting Classifier Algo\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "\n",
    "#For getting the Confusion Matrix,Accuracy of the Model,Loss etc.\n",
    "from sklearn.metrics import confusion_matrix,mean_squared_error,accuracy_score,classification_report\n",
    "from sklearn.metrics import precision_score,recall_score,roc_auc_score,f1_score,cohen_kappa_score\n",
    "\n",
    "## For Performing HyperParameter Tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "## To save the model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Application  to Solve the Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Analytics_Hack_Data:\n",
    "    def __init__(self,df):\n",
    "        ## To Get the Data\n",
    "        self.df = df\n",
    "        \n",
    "    def standardize_data(self,sdata):        \n",
    "        # initialising the MinMaxScaler\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        # learning the statistical parameters for each of the data and transforming\n",
    "        rescaled = scaler.fit_transform(sdata)\n",
    "        return rescaled ## Return the rescaled data\n",
    "    \n",
    "    def rescale_the_data(self,ImpCol):\n",
    "        ## Converting the date into number of days.\n",
    "        self.df['Dateofjoining'] = pd.to_datetime(self.df['Dateofjoining']).sub(pd.Timestamp('2010-01-01')).dt.days\n",
    "        ## converting Geder into integer\n",
    "        self.df['Gender'].replace('Female',0,inplace=True)\n",
    "        self.df['Gender'].replace('Male',1,inplace=True)\n",
    "        \n",
    "        ## converting City name and Education_level into integers\n",
    "        change={\n",
    "                    'Master':0,\n",
    "                    \"College\":1,\n",
    "                    \"Bachelor\":2\n",
    "                }\n",
    "        self.df['Education_Level'].replace(change,inplace=True)\n",
    "        self.df[\"City\"] = self.df[\"City\"].astype('category') ## Using category to convert categorical data into integers\n",
    "        self.df[\"City\"] = self.df[\"City\"].cat.codes\n",
    "        \n",
    "        size = len(self.df.columns) ## Finding number of columns\n",
    "        X = self.df.iloc[:,0:size-1] ## Getting all the columns except last one i.e. Target\n",
    "        X = X[ImpCol] ## Removing all the useless columns\n",
    "        Y = self.df.iloc[:,-1] ## Getting the Target Value\n",
    "        \n",
    "        rescaledX = self.standardize_data(X) ## Standardize or scale the data\n",
    "        \n",
    "        return rescaledX,Y ## Return all the pre processed data\n",
    "    \n",
    "    \n",
    "        \n",
    "    def Split_the_data(self,X,Y):\n",
    "        # Splitting the data into train and test\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y , train_size=0.7, random_state=42)\n",
    "        return X_train, X_test, y_train, y_test ## Return all the split data\n",
    "    \n",
    "    \n",
    "    def random_forest(self,X_train, X_test, y_train, y_test):\n",
    "        # classifier_rf = RandomForestClassifier(random_state=42, n_jobs=-1, max_depth=5,n_estimators=100, oob_score=True)\n",
    "\n",
    "        # n_estimators = number of desission trees, max_depth=depth of the tree\n",
    "        rf = RandomForestClassifier(n_estimators=300,max_depth=3) ## intiating the random forest \n",
    "        \n",
    "        ## Fitting the train data\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "        print(\"score on test: \" + str(rf.score(X_test, y_test)))\n",
    "        print(\"score on train: \"+ str(rf.score(X_train, y_train)))\n",
    "        \n",
    "        # save the model to disk\n",
    "        filename = 'random_forest.sav'\n",
    "        pickle.dump(rf, open(filename, 'wb'))\n",
    "        \n",
    "    \n",
    "    def Naive_Bayes(self,X_train, X_test, y_train, y_test):\n",
    "        ## Intiate the Naive Bayes\n",
    "        mnb = MultinomialNB().fit(X_train, y_train)  ## Fitting the train data\n",
    "        print(\"score on test: \" + str(mnb.score(X_test, y_test)))\n",
    "        print(\"score on train: \"+ str(mnb.score(X_train, y_train)))\n",
    "        # save the model to disk\n",
    "        filename = 'NaiveBayes.sav'\n",
    "        pickle.dump(mnb, open(filename, 'wb'))\n",
    "        \n",
    "        \n",
    "    def Logistic_Regression(self,X_train, X_test, y_train, y_test):\n",
    "        ### Intiate the Logistic regression Model\n",
    "        log = LogisticRegression(penalty='l2', dual=False, tol=1e-4,\n",
    "        C=1.0, fit_intercept=True,\n",
    "        intercept_scaling=1, class_weight=None,\n",
    "        random_state=None, solver='lbfgs',\n",
    "        max_iter=100, multi_class='auto',\n",
    "        verbose=0, warm_start=False, n_jobs=None)\n",
    "        \n",
    "        ## Fitting the train data\n",
    "        log.fit(X_train, y_train)\n",
    "        \n",
    "        print(\"score on test: \" + str(log.score(X_test, y_test)))\n",
    "        print(\"score on train: \"+ str(log.score(X_train, y_train)))\n",
    "        \n",
    "        # save the model to disk\n",
    "        filename = 'Logistic.sav'\n",
    "        pickle.dump(log, open(filename, 'wb'))\n",
    "        \n",
    "        \n",
    "    def K_neigbours(self,X_train, X_test, y_train, y_test):\n",
    "        \n",
    "        #knn = KNeighborsClassifier(n_neighbors=5,algorithm = 'ball_tree')\n",
    "        \n",
    "        knn = KNeighborsClassifier(algorithm = 'brute', n_jobs=-1)\n",
    "        \n",
    "        ## Fitting the train data\n",
    "        knn.fit(X_train, y_train)\n",
    "\n",
    "        print(\"score on test: \" + str(knn.score(X_test, y_test)))\n",
    "        print(\"score on train: \"+ str(knn.score(X_train, y_train)))\n",
    "        \n",
    "        \n",
    "    def support_vector(self,X_train, X_test, y_train, y_test):\n",
    "        svm=LinearSVC(C=0.0001)\n",
    "        \n",
    "        ## Fitting the train data\n",
    "        svm.fit(X_train, y_train)\n",
    "\n",
    "        ## save the model to disk\n",
    "        filename = 'Support_Vector.sav'\n",
    "        pickle.dump(svm, open(filename, 'wb'))\n",
    "        \n",
    "        print(\"score on test: \" + str(svm.score(X_test, y_test)))\n",
    "        print(\"score on train: \"+ str(svm.score(X_train, y_train)))\n",
    "    \n",
    "    def decision_tree(self,X_train, X_test, y_train, y_test):\n",
    "        clf = DecisionTreeClassifier(min_samples_split=10,max_depth=3)\n",
    "        \n",
    "        ## Fitting the train data\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        print(\"score on test: \" + str(clf.score(X_test, y_test)))\n",
    "        print(\"score on train: \"+ str(clf.score(X_train, y_train)))\n",
    "    \n",
    "    \n",
    "    def voting_classifier(self,X_train, X_test, y_train, y_test):\n",
    "\n",
    "        # 1) naive bias = mnb\n",
    "        mnb = MultinomialNB().fit(X_train, y_train)\n",
    "        # 2) logistic regression =lr\n",
    "        lr=LogisticRegression(max_iter=5000)\n",
    "        # 3) random forest =rf\n",
    "        rf = RandomForestClassifier(n_estimators=30,max_depth=3)\n",
    "        # 4) suport vecotr mnachine = svm\n",
    "        svm=LinearSVC(max_iter=5000)\n",
    "\n",
    "        evc=VotingClassifier(estimators=[('mnb',mnb),('lr',lr),('rf',rf),('svm',svm)])\n",
    "        \n",
    "        ## Fitting the train data\n",
    "        evc.fit(X_train, y_train)\n",
    "        \n",
    "        # save the model to disk\n",
    "        filename = 'voting_class.sav'\n",
    "        pickle.dump(mnb, open(filename, 'wb'))\n",
    "\n",
    "        print(\"score on test: \" + str(evc.score(X_test, y_test)))\n",
    "        print(\"score on train: \"+ str(evc.score(X_train, y_train)))\n",
    "        \n",
    "        \n",
    "    def grid_Serch(self,X_train, X_test, y_train, y_test):\n",
    "        ## initiate the models\n",
    "        rf = RandomForestClassifier(n_estimators=300,max_depth=3)\n",
    "        params = {\n",
    "            'max_depth': [2,3,5,10,20],\n",
    "            'min_samples_leaf': [5,10,20,50,100,200],\n",
    "            'n_estimators': [10,25,30,50,100,200]\n",
    "        }\n",
    "\n",
    "        # Instantiate the grid search model\n",
    "        grid_search = GridSearchCV(estimator=rf,\n",
    "                                   param_grid=params,\n",
    "                                   cv = 4,\n",
    "                                   n_jobs=-1, verbose=1, scoring=\"accuracy\")\n",
    "        \n",
    "        ## Fitting the train data\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        print(\"score on train: \"+ str(grid_search.score(X_train, y_train)))\n",
    "\n",
    "        #save the model to disk\n",
    "        filename = 'grid_search.sav'\n",
    "        pickle.dump(grid_search, open(filename, 'wb'))\n",
    "        \n",
    "    def accuracy_result(self,y_test, y_pred_test):\n",
    "        confusion_matrix=metrics.confusion_matrix(y_test, y_pred_test)\n",
    "        # USE THE IMPORTED CONFUSION MATRIX\n",
    "        print('\\n CONFUSION MATRIX:\\n ', confusion_matrix,'\\n')\n",
    "        TP = confusion_matrix[1, 1]\n",
    "        TN = confusion_matrix[0, 0]\n",
    "        FP = confusion_matrix[0, 1]\n",
    "        FN = confusion_matrix[1, 0]\n",
    "        false_positive_rate = round(FP / float(TN + FP),3)\n",
    "        print('FPR: ', false_positive_rate)\n",
    "        print('TPR/ RECALL/ SENSTIVITY: ', round(metrics.recall_score(y_test, y_pred_test), 3))\n",
    "        print('PRECISION:' ,round(metrics.precision_score(y_test, y_pred_test), 3))\n",
    "        specificity = round(TN / (TN + FP),3)\n",
    "        print('SPECIFICITY: ',specificity)\n",
    "        print('ACCURACY: ', np.round(metrics.accuracy_score(y_test, y_pred_test),3))\n",
    "        print('ROC AUC: ', np.round(roc_auc_score(y_test, y_pred_test),3))\n",
    "        print('Cohens kappa: ',np.round(cohen_kappa_score(y_test, y_pred_test),3))\n",
    "        print('F1 score: ', np.round(f1_score(y_test, y_pred_test),3))\n",
    "        print('\\n CLASSIFICATION REPORT: \\n',classification_report(y_test,y_pred_test))\n",
    "        return \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"New_Train.csv\") ## Reading the New_Train_csv File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    1616\n",
       "0     765\n",
       "Name: EmployeeLeaveOrNot, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.EmployeeLeaveOrNot.value_counts() ## Getting count of Target Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsampling_of_data(df):\n",
    "    data_major = df[df.EmployeeLeaveOrNot==1]\n",
    "    data_minor = df[df.EmployeeLeaveOrNot==0]\n",
    "    # Upsample minority class\n",
    "    data_minority_upsampled = resample(data_minor, \n",
    "                                         replace=True,     # sample with replacement\n",
    "                                         n_samples=int(len(data)*0.5),    # to match majority class for 12% proportion\n",
    "                                         random_state=123) # reproducible results\n",
    "\n",
    "    # Combine majority class with upsampled minority class\n",
    "    data_upsampled = pd.concat([data_major, data_minority_upsampled])\n",
    "        \n",
    "    return data_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    1616\n",
      "0    1190\n",
      "Name: EmployeeLeaveOrNot, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "sampled_data = upsampling_of_data(data) ## Calling upsampling of data.\n",
    "print(sampled_data.EmployeeLeaveOrNot.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikas.maurya\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype int8, int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\vikas.maurya\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       1\n",
      "2       1\n",
      "3       1\n",
      "5       1\n",
      "7       1\n",
      "       ..\n",
      "2067    0\n",
      "970     0\n",
      "2233    0\n",
      "2042    0\n",
      "203     0\n",
      "Name: EmployeeLeaveOrNot, Length: 2806, dtype: int64\n",
      "Fitting 4 folds for each of 180 candidates, totalling 720 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    4.4s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:    8.4s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:   16.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score on train: 0.9241344195519349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 720 out of 720 | elapsed:   25.2s finished\n"
     ]
    }
   ],
   "source": [
    "obj1 = Analytics_Hack_Data(sampled_data) ## initiate the object \n",
    "ImpCol = ['Age', 'Gender', 'City', 'Education_Level',\n",
    "       'Salary', 'Dateofjoining', 'Joining Designation',\n",
    "       'Designation', 'Total Business Value', 'Quarterly Rating'] ## important columns from the data\n",
    "x,y=obj1.rescale_the_data(ImpCol) ## calling rescaling of data\n",
    "print(y)\n",
    "X_train, X_test, y_train, y_test = obj1.Split_the_data(x,y) ## Calling plitting of data\n",
    "\n",
    "## Calling  the algorithm we wrote get model saved and get accuracy\n",
    "\n",
    "#obj1.random_forest(X_train, X_test, y_train, y_test)\n",
    "#obj1.Logistic_Regression(X_train, X_test, y_train, y_test)\n",
    "#obj1.Naive_Bayes(X_train, X_test, y_train, y_test)\n",
    "#obj1.voting_classifier(X_train, X_test, y_train, y_test)\n",
    "#obj1.support_vector(X_train, X_test, y_train, y_test)\n",
    "obj1.grid_Serch(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Converting the data to Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## converting the data to new train.\n",
    "def convert_new_Train(data):\n",
    "    gk = data.groupby(\"Emp_ID\") ## Group the data by Emp_ID\n",
    "    temp = gk.first()\n",
    "    mgk = gk.mean().round(2) ## Getting average of data\n",
    "    mgk['EmployeeLeaveOrNot'] = mgk['EmployeeLeaveOrNot'].apply(np.ceil) ## Solving the issue if data has floating value convert into integer\n",
    "    \n",
    "    ## appending all the missing i.e. Categorical  column to mgk since mean function applied to integers only\n",
    "    mgk[\"City\"] = temp[\"City\"]\n",
    "    mgk[\"Gender\"] = temp[\"Gender\"]\n",
    "    mgk[\"Education_Level\"] = temp[\"Education_Level\"]\n",
    "    mgk[\"Dateofjoining\"] = temp[\"Dateofjoining\"]\n",
    "    mgk.to_csv(\"New_Train.csv\") ## Saving new Train file in csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Train.csv\") ## Reading the Train data\n",
    "convert_new_Train(data) ## calling the convert_new_Train function we have made previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape ## Checking the shape of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the Submission .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vikas.maurya\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:334: DataConversionWarning: Data with input dtype int8, int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "Test = pd.read_csv(\"Test.csv\") ## Reading Test csv File\n",
    "temp = pd.DataFrame()  ## Generating blank datframe\n",
    "\n",
    "## finding the Emp_ID into new_train datasets\n",
    "for i in range(len(Test)):\n",
    "    temp1=data.loc[data['Emp_ID'] == Test[\"Emp_ID\"][i]] ## Getting rows with Emp_ID using Test file into Train\n",
    "    temp2=temp1.head(1) ## Getting the row\n",
    "    temp = temp.append(temp2, ignore_index = True) ## append into empty Dataframe\n",
    "    #print(Test[\"Emp_ID\"][i])\n",
    "\n",
    "df=temp ## Copying all the data into new DataFrame\n",
    "\n",
    "## Perform the Pre-Processing which we apply previosly\n",
    "df['Dateofjoining'] = pd.to_datetime(df['Dateofjoining']).sub(pd.Timestamp('2010-01-01')).dt.days\n",
    "df['Gender'].replace('Female',0,inplace=True)\n",
    "df['Gender'].replace('Male',1,inplace=True)\n",
    "change={\n",
    "                    'Master':0,\n",
    "                    \"College\":1,\n",
    "                    \"Bachelor\":2\n",
    " }\n",
    "df['Education_Level'].replace(change,inplace=True)\n",
    "df[\"City\"] = df[\"City\"].astype('category')\n",
    "df[\"City\"] = df[\"City\"].cat.codes\n",
    "df = df[ImpCol]\n",
    "# initialising the MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "# learning the statistical parameters for each of the data and transforming\n",
    "rescaled = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the mdel and predicting the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'random_forest.sav'\n",
    "# filename = 'Logistic.sav'\n",
    "# filename = 'voting_class.sav'\n",
    "# filename = \"Support_Vector.sav\"\n",
    "filename = 'grid_search.sav'    \n",
    "# load the model from disk\n",
    "\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "y_pred = loaded_model.predict(rescaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "       1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "       1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0,\n",
       "       1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0,\n",
       "       1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "       0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1,\n",
       "       1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt=pd.read_csv(\"temp.csv\") ## Lading Temp data which contains submission data\n",
    "tt[\"Target\"]=y_pred ## append the result we calculated using Model we trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.to_csv(\"subb.csv\") ## Convert into .csv file so that we can submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
